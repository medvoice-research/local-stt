{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whisper.cpp Transcription and Speaker Diarization for Kaggle\n",
    "\n",
    "This notebook demonstrates how to use [whisper.cpp](https://github.com/ggml-org/whisper.cpp) for offline speech recognition and speaker diarization. It provides functionality similar to the FastAPI service from the original project but adapted for Kaggle notebooks.\n",
    "\n",
    "## Features\n",
    "\n",
    "- **Multiple Models**: Support for various model sizes (tiny, base, small, medium, large)\n",
    "- **Diarization**: Speaker segmentation using pyannote.audio\n",
    "- **Offline Operation**: All processing happens locally\n",
    "- **On-demand Model Downloads**: Download models as needed\n",
    "- **Support for Various Audio Formats**: Through FFmpeg integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's start by installing the necessary dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q ffmpeg-python numpy torch tqdm requests python-dotenv kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU availability for PyTorch\n",
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Whisper.cpp\n",
    "\n",
    "We need to build whisper.cpp from source. Let's clone the repository and compile it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone whisper.cpp repository\n",
    "!git clone https://github.com/ggml-org/whisper.cpp.git\n",
    "\n",
    "# Build whisper.cpp\n",
    "%cd whisper.cpp\n",
    "!make\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Speaker Diarization Tools\n",
    "\n",
    "For speaker diarization, we'll use pyannote.audio. This requires a Hugging Face token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pyannote.audio for speaker diarization\n",
    "!pip install -q pyannote.audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Hugging Face token for pyannote.audio\n",
    "import os\n",
    "\n",
    "# You'll need to provide your Hugging Face token\n",
    "# Get one from: https://huggingface.co/settings/tokens\n",
    "# And accept the user agreement for: https://huggingface.co/pyannote/speaker-diarization-3.1\n",
    "\n",
    "HF_TOKEN = \"\" # Replace with your token\n",
    "\n",
    "# Uncomment below to set your token\n",
    "# HF_TOKEN = input(\"Enter your Hugging Face token: \")\n",
    "# os.environ[\"HF_TOKEN\"] = HF_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Structure\n",
    "\n",
    "Now let's set up the project structure with the necessary modules. We'll implement:\n",
    "\n",
    "1. Configuration\n",
    "2. Dataset download using kagglehub\n",
    "3. Transcription service\n",
    "4. Speaker diarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Define project root directory\n",
    "ROOT_DIR = Path(os.getcwd())\n",
    "logger.info(f\"ROOT_DIR is set to: {ROOT_DIR}\")\n",
    "\n",
    "# Whisper.cpp configuration\n",
    "WHISPER_BIN_PATH = ROOT_DIR / \"whisper.cpp\" / \"main\"\n",
    "DEFAULT_MODEL = \"small.en\"\n",
    "\n",
    "# Directories\n",
    "MODELS_DIR = ROOT_DIR / \"models\"\n",
    "TEMP_UPLOAD_DIR = ROOT_DIR / \"temp_uploads\"\n",
    "DATASET_DIR = ROOT_DIR / \"dataset\"\n",
    "\n",
    "# Ensure required directories exist\n",
    "def ensure_directories():\n",
    "    \"\"\"Create required directories if they don't exist.\"\"\"\n",
    "    for directory in [MODELS_DIR, TEMP_UPLOAD_DIR, DATASET_DIR]:\n",
    "        if not directory.exists():\n",
    "            directory.mkdir(parents=True, exist_ok=True)\n",
    "            logger.info(f\"Created directory: {directory}\")\n",
    "\n",
    "# Initialize directories\n",
    "ensure_directories()\n",
    "\n",
    "# Log configuration\n",
    "logger.info(f\"WHISPER_BIN_PATH: {WHISPER_BIN_PATH}\")\n",
    "logger.info(f\"DEFAULT_MODEL: {DEFAULT_MODEL}\")\n",
    "logger.info(f\"MODELS_DIR: {MODELS_DIR}\")\n",
    "logger.info(f\"TEMP_UPLOAD_DIR: {TEMP_UPLOAD_DIR}\")\n",
    "logger.info(f\"DATASET_DIR: {DATASET_DIR}\")\n",
    "logger.info(f\"HF_TOKEN: {'[SET]' if os.environ.get('HF_TOKEN') else '[NOT SET]'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dataset Download with Kagglehub\n",
    "\n",
    "Instead of managing model information directly, we'll use kagglehub to download a dataset with audio samples for testing transcription and diarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import glob\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "import subprocess\n",
    "import shutil\n",
    "import requests\n",
    "\n",
    "# Define the dataset to download\n",
    "KAGGLE_DATASET = \"wiradkp/mini-speech-diarization\"  # This dataset contains audio files with multiple speakers\n",
    "\n",
    "def download_kaggle_dataset():\n",
    "    \"\"\"Download a dataset from Kaggle using kagglehub\"\"\"\n",
    "    logger.info(f\"Downloading dataset from Kaggle: {KAGGLE_DATASET}\")\n",
    "    \n",
    "    try:\n",
    "        # Download the dataset\n",
    "        dataset_path = kagglehub.dataset_download(KAGGLE_DATASET)\n",
    "        logger.info(f\"Dataset downloaded to: {dataset_path}\")\n",
    "        \n",
    "        # Get list of audio files in the dataset\n",
    "        audio_files = []\n",
    "        for ext in ['*.wav', '*.mp3', '*.flac', '*.ogg']:\n",
    "            audio_files.extend(glob.glob(os.path.join(dataset_path, '**', ext), recursive=True))\n",
    "        \n",
    "        logger.info(f\"Found {len(audio_files)} audio files in the dataset\")\n",
    "        return dataset_path, audio_files\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to download Kaggle dataset: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Download whisper.cpp model\n",
    "def download_whisper_model(model_name=\"small.en\"):\n",
    "    \"\"\"Download a specific whisper.cpp model\"\"\"\n",
    "    script_path = MODELS_DIR / \"download-ggml-model.sh\"\n",
    "    \n",
    "    # Download the model download script if not exists\n",
    "    if not script_path.exists():\n",
    "        logger.info(f\"Downloading model download script to {script_path}\")\n",
    "        script_url = (\n",
    "            f\"https://raw.githubusercontent.com/ggml-org/whisper.cpp/\"\n",
    "            f\"master/models/download-ggml-model.sh\"\n",
    "        )\n",
    "        response = requests.get(script_url)\n",
    "        response.raise_for_status()\n",
    "        with open(script_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        # Make the script executable\n",
    "        script_path.chmod(0o755)\n",
    "    \n",
    "    model_file = MODELS_DIR / f\"ggml-{model_name}.bin\"\n",
    "    \n",
    "    if model_file.exists():\n",
    "        logger.info(f\"Model {model_name} already exists at {model_file}\")\n",
    "        return model_file\n",
    "    \n",
    "    logger.info(f\"Downloading model {model_name}...\")\n",
    "    try:\n",
    "        # Execute the download script\n",
    "        result = subprocess.run(\n",
    "            [str(script_path), model_name, str(MODELS_DIR)],\n",
    "            check=True,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "        )\n",
    "        logger.info(f\"Download output: {result.stdout}\")\n",
    "        \n",
    "        # Double-check the file exists after download\n",
    "        if not model_file.exists():\n",
    "            logger.error(f\"Download script completed but model file not found at {model_file}\")\n",
    "            raise RuntimeError(f\"Model file not found after download: {model_file}\")\n",
    "        \n",
    "        return model_file\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error downloading model: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Download the dataset\n",
    "try:\n",
    "    dataset_path, audio_files = download_kaggle_dataset()\n",
    "    print(f\"Dataset downloaded to: {dataset_path}\")\n",
    "    \n",
    "    if audio_files:\n",
    "        print(f\"Found {len(audio_files)} audio files. First few files:\")\n",
    "        for file in audio_files[:5]:\n",
    "            print(f\"- {os.path.basename(file)}\")\n",
    "    else:\n",
    "        print(\"No audio files found in the dataset. Downloading a sample file...\")\n",
    "        !wget -q -O sample.mp3 \"https://github.com/openai/whisper/raw/main/sample-en.mp3\"\n",
    "        audio_files = [str(ROOT_DIR / \"sample.mp3\")]\n",
    "        print(f\"Downloaded sample audio file: {audio_files[0]}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading dataset: {e}\")\n",
    "    # Create a fallback in case dataset download fails\n",
    "    print(\"Using a sample audio file as fallback...\")\n",
    "    !wget -q -O sample.mp3 \"https://github.com/openai/whisper/raw/main/sample-en.mp3\"\n",
    "    audio_files = [str(ROOT_DIR / \"sample.mp3\")]\n",
    "    print(f\"Downloaded sample audio file: {audio_files[0]}\")\n",
    "\n",
    "# Download the small model for our demonstration\n",
    "try:\n",
    "    model_path = download_whisper_model(\"small.en\")\n",
    "    print(f\"Downloaded whisper model: small.en to {model_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading model: {e}\")\n",
    "    # Try downloading a smaller model as fallback\n",
    "    try:\n",
    "        model_path = download_whisper_model(\"tiny.en\")\n",
    "        print(f\"Downloaded fallback model: tiny.en to {model_path}\")\n",
    "    except Exception as e2:\n",
    "        print(f\"Error downloading fallback model: {e2}\")\n",
    "        model_path = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Speaker Diarization Implementation\n",
    "\n",
    "Now we'll implement the speaker diarization service using pyannote.audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeakerDiarizationService:\n",
    "    def __init__(self, hf_token: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Initialize the speaker diarization service using pyannote/speaker-diarization.\n",
    "\n",
    "        Args:\n",
    "            hf_token: Hugging Face access token (required to use the pyannote models)\n",
    "        \"\"\"\n",
    "        self.pipeline = None\n",
    "        self.hf_token = hf_token if hf_token is not None else os.environ.get(\"HF_TOKEN\")\n",
    "        self._initialized = False\n",
    "\n",
    "        if not self.hf_token:\n",
    "            logger.warning(\n",
    "                \"No Hugging Face token provided for speaker diarization. \"\n",
    "                \"Set HF_TOKEN environment variable or provide token in constructor.\"\n",
    "            )\n",
    "\n",
    "    def initialize(self):\n",
    "        \"\"\"Initialize the pyannote speaker diarization pipeline\"\"\"\n",
    "        if self._initialized:\n",
    "            return\n",
    "\n",
    "        logger.info(\"Initializing pyannote speaker diarization pipeline\")\n",
    "        try:\n",
    "            # Import pyannote.audio here to avoid issues if not installed\n",
    "            from pyannote.audio import Pipeline\n",
    "            \n",
    "            # Load the speaker diarization model from Hugging Face\n",
    "            self.pipeline = Pipeline.from_pretrained(\n",
    "                \"pyannote/speaker-diarization-3.1\",\n",
    "                use_auth_token=self.hf_token,\n",
    "            )\n",
    "\n",
    "            # Use CUDA if available\n",
    "            if torch.cuda.is_available() and self.pipeline is not None:\n",
    "                logger.info(\"Using CUDA for speaker diarization\")\n",
    "                self.pipeline = self.pipeline.to(torch.device(\"cuda\"))\n",
    "\n",
    "            self._initialized = True\n",
    "            logger.info(\"Speaker diarization pipeline initialized successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize speaker diarization pipeline: {e}\")\n",
    "            raise\n",
    "\n",
    "    def diarize(\n",
    "        self,\n",
    "        audio_path: Union[str, Path],\n",
    "        num_speakers: Optional[int] = None,\n",
    "        min_speakers: Optional[int] = None,\n",
    "        max_speakers: Optional[int] = None,\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Perform speaker diarization on an audio file.\n",
    "\n",
    "        Args:\n",
    "            audio_path: Path to the audio file\n",
    "            num_speakers: Exact number of speakers in the audio (if known)\n",
    "            min_speakers: Minimum number of speakers expected\n",
    "            max_speakers: Maximum number of speakers expected\n",
    "\n",
    "        Returns:\n",
    "            A dictionary of diarization results\n",
    "        \"\"\"\n",
    "        if not self._initialized:\n",
    "            self.initialize()\n",
    "\n",
    "        try:\n",
    "            audio_path = str(audio_path) if isinstance(audio_path, Path) else audio_path\n",
    "\n",
    "            # Prepare input for pyannote\n",
    "            file = {\"uri\": \"audio\", \"audio\": audio_path}\n",
    "\n",
    "            # Set speaker count constraints if provided\n",
    "            diarization_params = {}\n",
    "            if num_speakers is not None:\n",
    "                diarization_params[\"num_speakers\"] = num_speakers\n",
    "            else:\n",
    "                if min_speakers is not None:\n",
    "                    diarization_params[\"min_speakers\"] = min_speakers\n",
    "                if max_speakers is not None:\n",
    "                    diarization_params[\"max_speakers\"] = max_speakers\n",
    "\n",
    "            # Apply diarization\n",
    "            if self.pipeline is None:\n",
    "                raise ValueError(\"Diarization pipeline not initialized properly\")\n",
    "            diarization = self.pipeline(file, **diarization_params)\n",
    "\n",
    "            # Convert pyannote Annotation to a more usable format\n",
    "            results = self._process_diarization(diarization)\n",
    "\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Speaker diarization failed: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _process_diarization(self, diarization):\n",
    "        \"\"\"\n",
    "        Process diarization results to a usable format.\n",
    "\n",
    "        Args:\n",
    "            diarization: PyAnnote diarization result\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with processed diarization segments\n",
    "        \"\"\"\n",
    "        segments = []\n",
    "\n",
    "        # Process each segment from the diarization result\n",
    "        for segment, track, speaker in diarization.itertracks(yield_label=True):\n",
    "            segments.append(\n",
    "                {\n",
    "                    \"speaker\": speaker,\n",
    "                    \"start\": segment.start,\n",
    "                    \"end\": segment.end,\n",
    "                    \"duration\": segment.end - segment.start,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # Sort segments by start time\n",
    "        segments.sort(key=lambda s: s[\"start\"])\n",
    "\n",
    "        return {\"segments\": segments, \"num_speakers\": len(diarization.labels())}\n",
    "\n",
    "    def align_diarization_with_transcription(\n",
    "        self, diarization_result: Dict, transcription_segments: List[Dict]\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Align speaker diarization results with whisper transcription segments.\n",
    "\n",
    "        Args:\n",
    "            diarization_result: Output from diarize() method\n",
    "            transcription_segments: List of segments from whisper transcription\n",
    "\n",
    "        Returns:\n",
    "            List of transcription segments with speaker labels\n",
    "        \"\"\"\n",
    "        if not diarization_result or not transcription_segments:\n",
    "            return transcription_segments\n",
    "\n",
    "        diarization_segments = diarization_result[\"segments\"]\n",
    "\n",
    "        # Create a function to find the best speaker for a given time range\n",
    "        def get_speaker_for_segment(start: float, end: float) -> str:\n",
    "            # Find overlapping diarization segments\n",
    "            overlaps = []\n",
    "            for segment in diarization_segments:\n",
    "                overlap_start = max(segment[\"start\"], start)\n",
    "                overlap_end = min(segment[\"end\"], end)\n",
    "\n",
    "                if overlap_end > overlap_start:  # There is an overlap\n",
    "                    overlap_duration = overlap_end - overlap_start\n",
    "                    overlaps.append((segment[\"speaker\"], overlap_duration))\n",
    "\n",
    "            if not overlaps:\n",
    "                return \"UNKNOWN\"\n",
    "\n",
    "            # Return the speaker with the most overlap\n",
    "            overlaps.sort(key=lambda x: x[1], reverse=True)\n",
    "            return overlaps[0][0]\n",
    "\n",
    "        # Assign speakers to transcription segments\n",
    "        for segment in transcription_segments:\n",
    "            start = segment.get(\"start\", segment.get(\"t0\"))\n",
    "            end = segment.get(\"end\", segment.get(\"t1\"))\n",
    "            if start is not None and end is not None:\n",
    "                segment[\"speaker\"] = get_speaker_for_segment(start, end)\n",
    "\n",
    "        return transcription_segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Transcription Service Implementation\n",
    "\n",
    "Now we'll implement the transcription service that integrates whisper.cpp with our speaker diarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import ffmpeg\n",
    "\n",
    "class TranscriptionService:\n",
    "    def __init__(self, model_path, whisper_bin=WHISPER_BIN_PATH, temp_dir=None, hf_token=None):\n",
    "        \"\"\"\n",
    "        Initialize the transcription service\n",
    "\n",
    "        Args:\n",
    "            model_path: Path to the whisper model file\n",
    "            whisper_bin: Path to the whisper binary\n",
    "            temp_dir: Directory to store temporary files\n",
    "            hf_token: Hugging Face API token for accessing pyannote models\n",
    "        \"\"\"\n",
    "        self.model_path = Path(model_path)\n",
    "        self.whisper_bin = whisper_bin\n",
    "        self.temp_dir = Path(temp_dir) if temp_dir is not None else TEMP_UPLOAD_DIR\n",
    "        self.hf_token = hf_token\n",
    "\n",
    "        # Initialize speaker diarization service if available\n",
    "        self.diarization_service = None\n",
    "        try:\n",
    "            self.diarization_service = SpeakerDiarizationService(hf_token=hf_token)\n",
    "            logging.info(\"Initialized pyannote speaker diarization service\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to initialize speaker diarization service: {e}\")\n",
    "\n",
    "        if not self.temp_dir.exists():\n",
    "            self.temp_dir.mkdir(parents=True)\n",
    "\n",
    "        if not self.model_path.exists():\n",
    "            raise FileNotFoundError(f\"Model file not found at {model_path}\")\n",
    "\n",
    "    def convert_audio_to_wav(self, audio_path):\n",
    "        \"\"\"Convert audio file to 16-bit WAV format required by whisper.cpp\"\"\"\n",
    "        output_path = self.temp_dir / f\"{Path(audio_path).stem}_converted.wav\"\n",
    "\n",
    "        try:\n",
    "            ffmpeg.input(audio_path).output(\n",
    "                str(output_path), acodec=\"pcm_s16le\", ar=16000, ac=1  # 16-bit PCM  # 16 kHz  # mono\n",
    "            ).run(quiet=True, overwrite_output=True)\n",
    "\n",
    "            logger.info(f\"Converted {audio_path} to {output_path}\")\n",
    "            return output_path\n",
    "        except ffmpeg.Error as e:\n",
    "            logger.error(f\"Error converting audio: {e.stderr.decode() if e.stderr else str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def transcribe(\n",
    "        self,\n",
    "        audio_path,\n",
    "        enable_diarization=False,\n",
    "        num_speakers=None,\n",
    "        min_speakers=None,\n",
    "        max_speakers=None,\n",
    "        language=\"auto\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Transcribe audio file using whisper.cpp\n",
    "\n",
    "        Args:\n",
    "            audio_path: Path to the audio file\n",
    "            enable_diarization: Whether to enable speaker diarization\n",
    "            num_speakers: Exact number of speakers (optional)\n",
    "            min_speakers: Minimum number of speakers (optional)\n",
    "            max_speakers: Maximum number of speakers (optional)\n",
    "            language: Language code or \"auto\" for detection\n",
    "\n",
    "        Returns:\n",
    "            A dictionary with the transcription results\n",
    "        \"\"\"\n",
    "        wav_path = None\n",
    "        try:\n",
    "            wav_path = self.convert_audio_to_wav(audio_path)\n",
    "\n",
    "            # Prepare command\n",
    "            cmd = [\n",
    "                str(self.whisper_bin),\n",
    "                \"-m\",\n",
    "                str(self.model_path),\n",
    "                \"-f\",\n",
    "                str(wav_path),\n",
    "                \"-oj\",  # Output JSON\n",
    "                \"-l\",\n",
    "                language,  # Use specified language or auto-detect\n",
    "            ]\n",
    "\n",
    "            cmd_str = \" \".join(cmd)\n",
    "            logger.info(f\"Running command: {cmd_str}\")\n",
    "\n",
    "            try:\n",
    "                result = subprocess.run(cmd, check=False, capture_output=True, text=True)\n",
    "            except FileNotFoundError:\n",
    "                logger.error(f\"Error: whisper binary not found at '{self.whisper_bin}'\")\n",
    "                return {\"error\": \"whisper binary not found. Please ensure whisper.cpp is built properly.\"}\n",
    "\n",
    "            # Get the output\n",
    "            output = result.stdout.strip()\n",
    "            logger.debug(f\"Raw output: {output}\")\n",
    "\n",
    "            if not output:\n",
    "                logger.error(\"No output from transcription command\")\n",
    "                return {\"error\": \"No output from transcription command: \" + result.stderr}\n",
    "\n",
    "            try:\n",
    "                # Try to parse as JSON first (if -oj flag worked as expected)\n",
    "                transcription_result = json.loads(output)\n",
    "                \n",
    "                # Apply pyannote diarization if requested and available\n",
    "                if enable_diarization and self.diarization_service is not None:\n",
    "                    logger.info(\"Applying pyannote speaker diarization\")\n",
    "                    try:\n",
    "                        diarization_result = self.diarization_service.diarize(\n",
    "                            audio_path=wav_path,\n",
    "                            num_speakers=num_speakers,\n",
    "                            min_speakers=min_speakers,\n",
    "                            max_speakers=max_speakers,\n",
    "                        )\n",
    "\n",
    "                        # Align diarization with transcription segments\n",
    "                        if \"segments\" in transcription_result:\n",
    "                            # Convert segment timestamps to seconds if needed\n",
    "                            for segment in transcription_result[\"segments\"]:\n",
    "                                if \"start\" not in segment and \"t0\" in segment:\n",
    "                                    segment[\"start\"] = segment[\"t0\"]\n",
    "                                if \"end\" not in segment and \"t1\" in segment:\n",
    "                                    segment[\"end\"] = segment[\"t1\"]\n",
    "\n",
    "                            # Add speaker labels to segments\n",
    "                            transcription_result[\n",
    "                                \"segments\"\n",
    "                            ] = self.diarization_service.align_diarization_with_transcription(\n",
    "                                diarization_result=diarization_result,\n",
    "                                transcription_segments=transcription_result[\"segments\"],\n",
    "                            )\n",
    "\n",
    "                            # Add diarization metadata\n",
    "                            transcription_result[\"diarization\"] = {\n",
    "                                \"num_speakers\": diarization_result[\"num_speakers\"],\n",
    "                                \"method\": \"pyannote\",\n",
    "                            }\n",
    "\n",
    "                            # Format the full text with speaker labels for better readability\n",
    "                            if \"text\" in transcription_result:\n",
    "                                speaker_texts = []\n",
    "                                for segment in transcription_result[\"segments\"]:\n",
    "                                    if \"speaker\" in segment and \"text\" in segment:\n",
    "                                        speaker_texts.append(\n",
    "                                            f\"{segment['speaker']}: {segment['text']}\"\n",
    "                                        )\n",
    "\n",
    "                                # Update the full text to include speaker information\n",
    "                                transcription_result[\"text_with_speakers\"] = \"\\n\".join(speaker_texts)\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Error during pyannote diarization: {e}\")\n",
    "\n",
    "                return transcription_result\n",
    "            except json.JSONDecodeError:\n",
    "                logger.info(\"Output is not JSON format, parsing as text\")\n",
    "                \n",
    "                # Simple fallback for non-JSON output\n",
    "                return {\"text\": output, \"error\": \"Failed to parse JSON output\"}\n",
    "\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            error_message = e.stderr if e.stderr else str(e)\n",
    "            logger.error(f\"Transcription failed: {error_message}\")\n",
    "            return {\"error\": f\"Transcription failed: {error_message}\"}\n",
    "        finally:\n",
    "            # Clean up temporary WAV file\n",
    "            if wav_path and os.path.exists(wav_path):\n",
    "                os.unlink(wav_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Process Audio Files from the Dataset\n",
    "\n",
    "Now we'll process a sample audio file from the dataset, applying both transcription and speaker diarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize transcription service with the small model\n",
    "if 'model_path' in locals() and model_path and 'audio_files' in locals() and audio_files:\n",
    "    # Set HF_TOKEN for pyannote.audio (if you have one)\n",
    "    if not HF_TOKEN:\n",
    "        print(\"WARNING: No HF_TOKEN provided. Speaker diarization may not work properly.\")\n",
    "        print(\"Consider providing a Hugging Face token for full functionality.\")\n",
    "    \n",
    "    transcription_service = TranscriptionService(\n",
    "        model_path=model_path,\n",
    "        whisper_bin=WHISPER_BIN_PATH,\n",
    "        hf_token=HF_TOKEN,\n",
    "    )\n",
    "    \n",
    "    # Process a sample audio file from the dataset\n",
    "    sample_file = audio_files[0]  # Use the first audio file\n",
    "    print(f\"Processing audio file: {os.path.basename(sample_file)}\")\n",
    "    \n",
    "    try:\n",
    "        # Transcribe with speaker diarization\n",
    "        result = transcription_service.transcribe(\n",
    "            audio_path=sample_file,\n",
    "            enable_diarization=True,\n",
    "            num_speakers=None,  # Let pyannote automatically detect number of speakers\n",
    "            language=\"auto\",    # Auto-detect language\n",
    "        )\n",
    "        \n",
    "        # Display results\n",
    "        print(\"\\n=== Transcription Results ===\")\n",
    "        \n",
    "        if \"error\" in result:\n",
    "            print(f\"Error: {result['error']}\")\n",
    "        else:\n",
    "            print(\"\\nFull transcription:\")\n",
    "            print(result.get(\"text\", \"No text found\"))\n",
    "            \n",
    "            print(\"\\nTranscription with speaker labels:\")\n",
    "            if \"text_with_speakers\" in result:\n",
    "                print(result[\"text_with_speakers\"])\n",
    "            else:\n",
    "                print(\"No speaker-labeled text available\")\n",
    "            \n",
    "            print(\"\\nDetailed segments:\")\n",
    "            for i, segment in enumerate(result.get(\"segments\", [])[:10]):  # Show first 10 segments\n",
    "                start = segment.get(\"start\", segment.get(\"t0\", 0))\n",
    "                end = segment.get(\"end\", segment.get(\"t1\", 0))\n",
    "                speaker = segment.get(\"speaker\", \"Unknown\")\n",
    "                print(f\"{i+1}. [{start:.2f}s - {end:.2f}s] Speaker {speaker}: {segment.get('text', '')}\")\n",
    "            \n",
    "            if len(result.get(\"segments\", [])) > 10:\n",
    "                print(f\"... and {len(result.get('segments', [])) - 10} more segments\")\n",
    "            \n",
    "            if \"diarization\" in result:\n",
    "                print(f\"\\nDetected {result['diarization'].get('num_speakers', 0)} speakers\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing audio file: {e}\")\n",
    "else:\n",
    "    print(\"Cannot process audio files: Either no model or no audio files available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Visualize Diarization Results\n",
    "\n",
    "Let's create a visualization of the diarization results to better understand the speaker segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the diarization results\n",
    "if 'result' in locals() and 'segments' in result and not \"error\" in result:\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        import numpy as np\n",
    "        from matplotlib.patches import Rectangle\n",
    "        \n",
    "        # Extract speakers and segments\n",
    "        speakers = set()\n",
    "        for segment in result['segments']:\n",
    "            if 'speaker' in segment:\n",
    "                speakers.add(segment['speaker'])\n",
    "        \n",
    "        # Convert speaker labels to numeric IDs for plotting\n",
    "        speaker_ids = {speaker: i for i, speaker in enumerate(sorted(speakers))}\n",
    "        \n",
    "        # Create plot\n",
    "        fig, ax = plt.subplots(figsize=(15, 5))\n",
    "        \n",
    "        # Plot each segment\n",
    "        for segment in result['segments']:\n",
    "            if 'speaker' in segment and 'start' in segment and 'end' in segment:\n",
    "                speaker = segment['speaker']\n",
    "                start = segment['start']\n",
    "                end = segment['end']\n",
    "                duration = end - start\n",
    "                speaker_id = speaker_ids[speaker]\n",
    "                \n",
    "                # Plot rectangle for this segment\n",
    "                rect = Rectangle((start, speaker_id - 0.4), duration, 0.8, \n",
    "                                alpha=0.6, color=f'C{speaker_id % 10}')\n",
    "                ax.add_patch(rect)\n",
    "                \n",
    "                # Add text\n",
    "                if duration > 1.0:  # Only add text for segments long enough to be readable\n",
    "                    ax.text(start + duration/2, speaker_id, segment['text'][:20] + ('...' if len(segment['text']) > 20 else ''), \n",
    "                           ha='center', va='center', fontsize=8)\n",
    "        \n",
    "        # Set labels and title\n",
    "        ax.set_yticks(list(speaker_ids.values()))\n",
    "        ax.set_yticklabels(list(speaker_ids.keys()))\n",
    "        ax.set_xlabel('Time (seconds)')\n",
    "        ax.set_ylabel('Speaker')\n",
    "        ax.set_title('Speaker Diarization Results')\n",
    "        \n",
    "        # Set x-axis limit to match the audio duration\n",
    "        max_time = max([segment.get('end', segment.get('t1', 0)) for segment in result['segments']])\n",
    "        ax.set_xlim(0, max_time)\n",
    "        \n",
    "        # Set y-axis limit\n",
    "        ax.set_ylim(-1, len(speakers))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Create a text visualization too\n",
    "        print(\"\\n=== Timeline Visualization ===\")\n",
    "        print(\"Time (s) | Speaker | Text\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for segment in sorted(result['segments'], key=lambda s: s.get('start', s.get('t0', 0))):\n",
    "            start = segment.get('start', segment.get('t0', 0))\n",
    "            end = segment.get('end', segment.get('t1', 0))\n",
    "            speaker = segment.get('speaker', 'Unknown')\n",
    "            text = segment.get('text', '')\n",
    "            print(f\"{start:7.2f} - {end:5.2f} | {speaker:7} | {text}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating visualization: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates how to use whisper.cpp for offline transcription combined with pyannote.audio for speaker diarization. By downloading audio samples from a Kaggle dataset, we can test the full pipeline without needing to upload our own files.\n",
    "\n",
    "Key features implemented:\n",
    "\n",
    "1. **whisper.cpp integration**: Fast, offline speech recognition\n",
    "2. **Speaker diarization**: Using pyannote.audio's state-of-the-art speaker segmentation\n",
    "3. **Kaggle dataset integration**: Easy access to test audio files\n",
    "4. **Visualization**: Both text and graphical representation of speaker turns\n",
    "\n",
    "To adapt this for your own use:\n",
    "\n",
    "- Try different whisper.cpp models (tiny, base, small, medium, large) based on your accuracy needs\n",
    "- Experiment with speaker count constraints if you know how many speakers to expect\n",
    "- Use your own audio files by replacing the sample file paths"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
